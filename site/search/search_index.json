{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Menurut bahasa, data merupakan bentuk jamak dari kata datum (bahasa latin) yang berarti sesuatu yang diberikan . Menurut istilah, pengertian data adalah kumpulan informasi atau keterangan-keterangan yang diperoleh dari pengamatan, informasi itu bisa berupa angka, lambang atau sifat. Dalam kehidupan sehari-hari data berarti suatu pernyataan yang diterima secara apa adanya. Artinya data yang diperoleh dari berbagai sumbernya masih menjadi sebuah anggapan atau fakta karena memang belum diolah lebih lanjut. Setelah diolah melaui suatu penelitian atau percobaan maka data dapat berubah menjadi bentuk yang lebih kompleks misal database, informasi atau bahkan solusi pada masalah tertentu. Data yang telah diolah akan berubah menjadi sebuah informasi yang dapat digunakan untuk menambah pengetahuan bagi yang menerimanya. Maka dalam hal ini data dapat dianggap sebagai obyek dan informasi adalah suatu subyek yang bermanfaat bagi penerimanya. Informasi juga bisa disebut sebagai hasil pengolahan ataupun pemrosesan data. Oleh karena itu artikel ini akan membahas tutorial untuk mengolah data yang diperoleh dari crawling web sehingga diperoleh suatu informasi. Data yang didapat dari proses crawl akan diolah menggunakan metode Fuzzy C-Means untuk melakukan cluster pada data dan Silhouette Coefficient digunakan untuk menguji kualitas cluster yang diperoleh. Hasil akhir yang ingin dicapai pada artikel ini adalah membandingkan jumlah cluster sehingga mendapatkan jumlah cluster yang efiseien dan berkualitas berdasarkan perolehan skor dari Silhouette Coefficient . Selain itu pada tutorial ini juga terdapat proses Feature Selection untuk mengurangi jumlah fitur agar tidak memakan waktu komputasi yang banyak. Algoritma yang digunakan pada proses ini adalah Drop High Correlation , algortima ini digunakan untuk menghindari membuang fitur yang memiliki informasi yang diperlukan karena algoritma ini melakukan proses korelasi matrik untuk mengetahui fitur mana yang tidak memiliki korelasi dengan fitur yang lainnya. Sehingga algoritma ini dapat mencegah membuang fitur yang penting. Step 1. Installing Dalam tutorial ini semua proses akan dibangun menggunakan bahasa pemograman Python . Alasan mengapa author menggunakan bahasa pemograman ini karena Python merupakan bahasa pemograman yang populer akhir-akhir ini, selain itu Python menyediakan banyak library yang mudah digunakan dalam membangun suatu sistem. Adapun library yang digunakan akan dijelaskan pada setiap stepnya. Panduan instalasi python dapat teman-teman baca disini Panduan instalasi library atau package dapat teman-teman disini Step 2. Crawling Apa itu Web Crawler ? Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Umumnya Web Crawler dapat digunakan berkaitan dengan Search Engine , yakni mengumpulkan informasi mengenai apa yang ada pada halaman-halaman web publik. Bagaimana cara kerja Web Crawler ? Ketika Web Crawler suatu Search Engine mengunjungi halaman web, Web Crawler akan membaca teks , hyperlink dan macam-macam tag konten yang digunakan dalam situs misalnya meta tag yang berisi banyak keyword. Data tersebut kemudian akan dimasukkan ke dalam database atau tempat penyimpanan. Apa yang akan dilakukan ? Install library yang digunakan from bs4 import BeautifulSoup import requests BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web. Menentukan halaman web yang akan di crawl Pada artikel ini halaman web yang akan di crawl datanya adalah https://sports.okezone.com/. Menentukan informasi apa yang akan di simpan Setelah menentukan halaman web yang akan di crawl, selanjutnya berdasarkan informasi yang ada di halaman web diatas, data artikel atau berita yang akan di simpan pada tutorial ini berupa URL, judul dan isi berita . Proses Crawling >> Membuat requests pada halaman web yang dituju req = requests.get('http://lintasperistiwa.com/') >> Menentukan berita yang akan diambil dari halaman web Berita yang akan diambil adalah yang terletak di panel tengah pada list berita terkini, berita ini merupakan berita yang diupload tiap hari. Untuk dapat mengakses tag html dari list berita tersebut, dapat dilakukan dengan inspect element , yakni dengan cara : klik kanan pada halaman web pilih inspect maka akan muncul panel inspect di jendela sebelah kanan Selanjutnya pada panel inspect , cari tag html yang memuat list berita pada halaman web. Biasanya list berita memiliki nama class yang sama pada setiap berita. Pada halaman web diatas contohnya adalah class item-outer . news_links = soup.find_all(\"div\",{'class':'vel-frame-post-block'}) >>Mendapatkan judul dan alamat web dari isi artikel Setelah tag html list berita telah diperoleh, selanjutnya adalah mencari tag html yang memuat judul dan link dari judul tersebut. Hal ini dilakukan agar menelusuri isi dari setiap list berita yang terdapat pada halaman tersebut. Biasanya tag html yang memuat link terdapat attribut href . Dari list berita tersebut, jika ditelusuri lebih mendalam tag html yang memuat link berita memiliki class ellipsis2 . for idx,news in enumerate(news_links): #find news title title_news= news.find('a',{'class':'mod-judul-post'}).text #find urll news url_news = news.find('a',{'class':'mod-judul-post'}).get('href') Dilakukan perulangan untuk membaca semua link yang terdapat pada list berita. >>Membuat requests pada setiap berita #find news content in url req_news = requests.get(url_news) soup_news = BeautifulSoup(req_news.text, \"lxml\") Setelah mendapatkan link setiap berita, sama seperti langkah sebelumnya hal yang dilakukan adalah melakukan requests pada setiap berita. >> Mendapatkan isi artikel pada berita Sama seperti langkah sebelumnya, yang harus dilakukan selanjutnya adalah melakukan Inspect pada salah satu berita kemudiah mancari tag html yang mengandung isi dari berita tersebut. #find news content news_content = soup_news.find(\"div\",{'class':'entry-content'}) #find paragraph in news content p = news_content.find_all('p') content = ' '.join(item .text for item in p) news_content = content.encode('utf8','replace') Setelah mendapatkan isi dari suatu berita yang terdapat pada tag p selanjutnya menggabungkan isi semua tag tersbut menjadi sebuah string. Note : program diatas tidak dapat digunakan pada halaman web yang lain. Alasannya karena setiap web memiliki tag html yang berbeda. Jadi jika ingin melakukan crawling pada halaman web yang lain, teman-teman harus menyesuaikan dengan tag htmlnya dengan melakukan inspect yang telah dicontohkan diatas. Menyimpan data dalam bentuk excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] baris = 1 if(len(news_content)>5): sheet.cell(row=baris, column=1).value = baris sheet.cell(row=baris, column=2).value = url_news sheet.cell(row=baris, column=3).value = title_news sheet.cell(row=baris, column=4).value = str(news_content) baris+=1 woorkbook.save('data_crawling.xlsx') Libary yang digunakan untuk mengolah data menggunakan excel adalah openpyxl . Library ini sangat mudah untuk mengakses data pada excel, setelah melakukan inisialisasi woorkbook = openpyxl.Workbook() dan mengakses sheet yang akan digunakan sheet = woorkbook['Data'] , data dapat disimpan berdasarkan baris dan kolom pada excel ` sheet.cell(row=baris, column=1).value = baris sheet.cell(row=baris, column=2).value = url_news sheet.cell(row=baris, column=3).value = title_news sheet.cell(row=baris, column=4).value = str(news_content) baris+=1` . Setelah semua baris dan kolom sudah diberikan nilai maka selanjutnya adalah menyimpan file excel dengan woorkbook.save('data_crawling.xlsx') . Cara akses woorkbook.save('data_crawling.xlsx') url = 'http://lintasperistiwa.com/' crawl = crawl_lintasperistiwa(url) List akan disimpan dengan nama crawl di directory data_log sedangkan file excel akan disimpan di directory excel_log . Sehingga sebelum code dijalankan, buatlah kedua folder tersebut terlebih dahulu pada directory yang sama dengan file .py Notes : dalam beberapa kasus akan muncul pesan error lxml saat code dijalankan, jika demikian yang harus dilakukan adalah menginstall library lxml. Hasil Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca Step 3. Preprocessing Apa itu Text Preprocessing ? Berdasarkan ketidak teraturan struktur data teks, maka proses sistem temu kembali informasi ataupun text mining memerlukan beberapa tahap awal yang pada intinya adalah mempersiapkan agar teks dapat diubah menjadi lebih terstruktur. Salah satu implementasi dari text mining adalah tahap Text Preprocessing. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi (1) case folding, (2) tokenizing, (3) filtering, dan (4) stemming. Apa yang akan dilakukan ? Install library yang digunakan import openpyxl import re from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory import pandas as pd import numpy as np Library yang digunakan pada text preprocessing ialah re digunakan untuk symbol removing , nltk digunakan untuk tokenizing dan stopword, Sastrawi digunakan untuk stemming. Symbol Removing data2 = [] for i in range(len(data)): data2.append(re.sub(r'[^\\w]',' ',str(data[i]))) Tokenizing data5 = [] woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data4)): token = data4[i].split() data5.append(token) sheet.cell(row=i+1, column=1).value = i+1 for j in range(len(token)): sheet.cell(row=i+1, column=2+j).value = token[j] woorkbook.save('tokenisasi.xlsx') Dokumen teks terdiri dari sekumpulan kalimat, proses tokenisasi memecah dokumen tersebut menjadi bagian-bagian kata yang disebut token. Contohnya kalimat \"saya menyapu halaman di depan rumah\" setelah ditokenisasi menjadi sebuah list [\"saya\", \"memakan\", \"nasi\", \"di\", \"dalam\", \"dapur\"] . Stopword data3 = [] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): stop = stopword.remove(data2[i]) data3.append(stop) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = stop woorkbook.save('stopwords.xlsx') Selanjutnya ialah mengambil kata-kata yang dianggap penting dari hasil tokenization atau membuang kata-kata yang dianggap tidak terlalu mempunyai arti penting dalam proses text mining. Contohnya data [\"saya\", \"makan\", \"nasi\", \"di\", \"dalam\", \"dapur\"] menjadi [\"saya\", \"makan\", \"nasi\", \"dalam\", \"dapur\"] . Stemming data4 = [] factory = StemmerFactory() stemmer = factory.create_stemmer() woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): katadasar = stemmer.stem(data3[i]) data4.append(katadasar) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = katadasar woorkbook.save('stemming.xlsx') Stemming bertujuan untuk mentransformasikan kata menjadi kata dasarnya (root word) dengan menghilangkan semua imbuhan kata. Contohnya data [\"saya\", \"makan\", \"nasi\", \"dalam\", \"dapur\"] menjadi [\"saya\", \"makan\", \"nasi\", \"dalam\", \"dapur\"] . Cara akses Sebelum melakukan proses diatas, terlebih dahulu membaca data hasil crawling yang telah disimpan dalam bentuk list dengan cara dibawah ini. def load_list(name): with open(\"data_logs/\"+name+\".txt\", \"rb\") as fp: # Unpickling b = pickle.load(fp) print(\"Success to load.....\") return b Selanjutnya, pergunakan semua function pada step ini secara bersama. print(\"Load data......\") data = load_list(\"data\") print(\"Lowercase......\") lowercase = lowercase(data) print(\"Remove symbol......\") symbol_remover = symbol_remover(lowercase) print(\"Tokenisasi......\") tokenisasi = tokenisasi(symbol_remover) print(\"Stopword......\") stopword_s = stopword_s(tokenisasi) print(\"Stemming......\") stemming = stemming(stopword_s) Pada hasil akhir dari tutorial ini, kita akan membandingkan uni-gram dan bi-gram. Uni-gram yakni hasil token merupakan tiap satu suku kata, sedangkan bi-gram hasil token menjadi 2 kata. Contoh bi-gram [\"saya makan\", \"makan nasi\", \"nasi dalam\", \"dalam dapur\"]. Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini Step 4. Fiture Selection Apa itu Fiture Selection ? Pemilihan istilah untuk dijadikan indeks merupakan isu yang penting dalam sistem temu-kembali informasi. Selanjunya proses pemilihan istilah ini disebut dengan seleksi fitur (feature selection). Mengapa Fiture Selection penting ? Fitur seleksi dapat menyebabkan berkurangnya ukuran indeks sehingga proses retrieval suatu dokumen menjadi lebih cepat sebab jumlah indeks yang dicari menjadi lebih sedikit. Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Apa itu Drop High Correlation ? Korelasi adalah istilah statistik yang dalam penggunaan umum mengacu pada seberapa dekat dua variabel untuk memiliki hubungan linier satu sama lain. Fitur dengan korelasi tinggi lebih linear dan karenanya memiliki efek yang hampir sama pada variabel dependen. Jadi, ketika dua fitur memiliki korelasi tinggi, kita dapat menjatuhkan ( drop ) salah satu dari dua fitur tersebut. Penghapusan fitur yang berbeda dari dataset akan memiliki efek yang berbeda pada nilai p untuk dataset. Kami dapat menghapus fitur yang berbeda dan mengukur nilai p di setiap kasus. Nilai-p yang diukur ini dapat digunakan untuk memutuskan apakah akan mempertahankan fitur atau tidak. Untuk informasi lebih lanjut mengenai perhitungan matrik korelasi bisa teman-teman baca disini Apa yang akan dilakukan ? Install library yang digunakan import pandas as pd import numpy as np Library yang digunakan pada seleksi fitur ialah pandas digunakan untuk membuat data menjadi data frame dan numpy digunakan untuk menjadikan data menjadi matrix dan mengolahnya. Proses Fiture Selection >> Mengubah data menjadi dataframe Data yang berbentuk array dirubah menjadi dataframe , yakni struktur data tabel dua dimensi yang memiliki label. Hal ini akan memudahkan dalam operasi baris dan kolom. df = pd.DataFrame(X) >> Membuat korelasi matriks Dari matriks korelasi ini dapat dilihat bahwa pada bagian diagonal, nilainya pasti = 1 sebab variabel tersebut berkorelasi sempurna positif dengan dirinya sendiri. Nilai korelasi antara pasangan variabel pada set data1 dan data2, menunjukkan nilai dalam range -1 dan 1. Ini disebabkan oleh standarisasi menggunakan standar deviasi sebagai pembagi pada saat perhitungan hasil korelasinya. corr_matrix = df.corr().abs() >> Menentukan fitur yang akan dihapus Setelah mendapatkan matrik kolerasi, selanjutnya menentukan kolom yang memiliki nilai lebih dari 0.95 . Nilai ini fleksibel, bergantung pada batas nilai fitur yang dianggap penting. to_drop = [column for column in upper.columns if any(upper[column] > 0.95)] >> Menghapus fitur Proses selanjutnya ialah menghapus fitur yang dianggap tidak penting berdasarkan nilai batas yang ditentukan pada proses sebelumnya. new_data = df.drop(df.columns[to_drop], axis=1) Full Code import openpyxl import re from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory #n-gram def generate_ngrams(data,n): n_gram = [] for i in range(0,len(data)): tokens = data[i] ngrams = zip(*[tokens[i:] for i in range(n)]) n_gram.append([\" \".join(ngram) for ngram in ngrams]) return n_gram #read data from excel book = openpyxl.load_workbook('crawling.xlsx') sheet = book['Data'] row_count = sheet.max_row data = [] for i in range(row_count): data.append(sheet.cell(row=i+1, column=4).value) #symbol remove data2 = [] for i in range(len(data)): data2.append(re.sub(r'[^\\w]',' ',str(data[i]))) #stopword kata sambung data3 = [] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): stop = stopword.remove(data2[i]) data3.append(stop) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = stop woorkbook.save('stopwords.xlsx') #stemming kata imbuan data4 = [] factory = StemmerFactory() stemmer = factory.create_stemmer() #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): katadasar = stemmer.stem(data3[i]) data4.append(katadasar) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = katadasar woorkbook.save('stemming.xlsx') #tokenisasi kata pemisah data5 = [] #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data4)): token = data4[i].split() data5.append(token) sheet.cell(row=i+1, column=1).value = i+1 for j in range(len(token)): sheet.cell(row=i+1, column=2+j).value = token[j] woorkbook.save('tokenisasi.xlsx') #n-gram data6 = generate_ngrams(data5,2) print(len(data5)) #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(0,len(data6)): sheet.cell(row=i+1, column=1).value = i+1 for j in range(0,len(data6[i])): sheet.cell(row=i+1, column=2+j).value = data6[i][j] print(data6[i][j]) woorkbook.save('n_gram.xlsx') #VSM mencari data yang sama term = [] #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] j=2 for i in range(len(data5)): for word in data5[i]: if word not in term : term.append(word) sheet.cell(row=1, column=j).value = word j+=1 #count word in document for i in range(len(data5)): sheet.cell(row=2+i, column=1).value = i+1 j = 2 for word in term : sheet.cell(row=2+i, column=j).value = data5[i].count(word) j+=1 woorkbook.save('VSM.xlsx') Cara akses collecting_fiture merupakan label dari setiap kolom atau list yang terdiri dari semua kata yang terdapat pada semua berita dimana tidak ada kata yang sama pada list. Panjang collecting_fiture sama dengan panjang kolom vsm . Sedangkan vsm adalah banyaknya kemunculan kata yang ada di collecting_fiture pada setiap berita. Sehingga diperoleh matrik 2-dimensi. term = [] data7 = [] fitur = [] woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] j=2 for i in range(len(data5)): for word in data5[i]: if word not in term : term.append(word) sheet.cell(row=1, column=j).value = word fitur.append(word) j+=1 for i in range(len(data5)): sheet.cell(row=2+i, column=1).value = i+1 j = 2 tamporary = [] for word in term : sheet.cell(row=2+i, column=j).value = data5[i].count(word) tamporary.append(data5[i].count(word)) j+=1 data7.append(tamporary) woorkbook.save('VSM.xlsx') Sehingga vsm dan collecting_fiture digunakan sebagai input drop_highly_correlation . #Uni Gram vsm = load_list(\"vsm\") collecting_fiture = load_list(\"collecting_fiture\") new_vsm,new_collecting_fiture = drop_highly_correlation(vsm,collecting_fiture) #Bi Gram vsm_n_gram = load_list(\"vsm_2\") collecting_fiture_n_gram = load_list(\"collecting_fiture_2\") new_vsm_n_gram,new_collecting_fiture_n_gram = drop_highly_correlation(vsm_n_gram,collecting_fiture_n_gram) hasil Hasil dari seleksi fitur ini adalah jumlah fitur atau atribut atau kolom pada data, baik uni-gram dan bi-gram akan berkurang. Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini Step 5. TF-IDF Apa itu TF-IDF ? Metode TF-IDF merupakan metode untuk menghitung bobot setiap kata yang paling umum digunakan pada information retrieval. Metode ini juga terkenal efisien, mudah dan memiliki hasil yang akurat. Metode ini akan menghitung nilai Term Frequency (TF) dan Inverse Document Frequency (IDF) pada setiap token (kata) di setiap dokumen dalam korpus. Apa yang akan dilakukan ? Install library yang digunakan import math Library yang digunakan ialah math yang digunakan untuk melakukan proses perhitungan. TF TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. def tf(data,term): new_data=[] for i in range(len(data)): tempt = [] for word in term: tempt.append(data[i].count(word)) new_data.append(tempt) return new_data IDF IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. def idf(data): new_data = [] for i in range(len(data[0])): count = 0 for j in range(len(data)): count+= int(data[j][i]) new_data.append(math.log10(len(data)/count)) return new_data TF-IDF def tf_idf(tf,idf): new_data = [] for i in range(len(tf)): tempt = [] for j in range(len(tf[i])): tempt.append(int(tf[i][j])*int(idf[j])) new_data.append(tempt) return new_data Cara akses #Uni Gram stemming = load_list(\"stemming\") collecting_fiture = load_list(\"new_collecting_fiture\") print(\"Load Tf....\") tf_a = tf(stemming,collecting_fiture) print(\"Load idf...\") idf_a = idf(tf_a) print(\"Load tf_idf...\") tf_idf_a = tf_idf(tf_a,idf_a,collecting_fiture) #Bi Gram n_gram = load_list(\"token_2\") collecting_fiture_n_gram = load_list(\"new_collecting_fiture_2\") print(\"Load Tf_n_gram....\") tf_n_gram = tf(n_gram,collecting_fiture_n_gram) print(\"Load idf_n_gram...\") idf_n_gram = idf(tf_n_gram) print(\"Load tf_idf_n_gram...\") tf_idf_n_gram = tf_idf(tf_n_gram,idf_n_gram,collecting_fiture_n_gram) Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini Step 6. Clustering Apa itu Clustering ? Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Apa itu Fuzzy C-Means ? Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Untuk informasi lebih lanjut mengenai perhitungan Fuzzy C-Means , silahkan baca disini Apa itu Silhouette Coefficient ? ilhouette coefficient berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Untuk informasi lebih lanjut mengenai perhitungan silhoutte coeffisien , silahkan baca disini Apa yang akan dilakukan ? Install library yang digunakan import numpy as np from sklearn.metrics import silhouette_score import skfuzzy as fuzz Library yang digunakan ialah numpy yang digunakan untuk melakukan proses perhitungan, sklearn yang digunakan untuk menghitung nilai silhouette dan skfuzzy yang digunakan untuk melakukan proses perhitungan fuzzy c-means . Fuzzy C-means cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) Untuk menghitung fuzzy c-means telah disediakan oleh library python, adapun inputnya yakni : data berupa array 2d merupakan data yang akan dikelompokkan. jumlah cluster atau kelas yang diinginkan. array eksponensial batas error jumlah maksimum iterasi Sedangkan outputnya berupa : cntr merupakan pusat cluster u merupakan matriks partisi-c fuzzy akhir u0 merupakan tebakan awal pada fuzzy c-partisied matrix distant merupakan matriks jarak euclidian fObj menrupakan riwayat fungsi obyektif iterasi merupakan jumlah iterasi yang dijalankan fpc: merupakan koefisien partisi fuzzy akhir. Yang akan digunakan pada tutorial ini adalah u Silhouette Coefficient silhouette_score(data, membership, random_state=10) Setelah setiap data telah di cluster , selanjutnya adalah menghitung silhouette score . Silhouette score dihitung pada setiap cluster, baik itu untuk uni-gram maupun bi-gram . Full Code def clustering(data): s_avg = [] for i in range(2,len(data)): cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) s_avg.append(silhouette_score(data, membership, random_state=10)) return s_avg Cara akses #Uni Gram tfidf = load_list(\"tf_idf\") s_avg = clustering(tfidf) #Bi Gram tfidf = load_list(\"tf_idf_2\") s_avg = clustering(tfidf) Hasil Setelah diperoleh silhouette score untuk masing-masing cluster pada uni-gram dan bi-gram dengan menggunakan perintah dibawah : #Uni Gram s_avg = func.load_list(\"s_avg\") print(s_avg.index(max(s_avg))+2) #Bi Gram s_avg = func.load_list(\"s_avg_2\") print(s_avg.index(max(s_avg))+2) Diperoleh bahwa cluster yang memiliki silhouette score paling tinggi adalah 2. Conclusion Hasil yang diperoleh pada percobaan di atas mungkin bisa dikatakan tidak terlalu memiliki hasil yang baik, dan mungkin code line yang ada masih terlalu panjang. Meski tutorial ini masih banyak kekurangan, semoga dapat memberikan manfaat untuk teman-teman semua. Jika ada masukan atau perbaikan yang memberikan hasil yang lebih baik atau yang memiliki code program yang lebih pendek, jangan sungkan untuk memberikan saran kalian melalui email. Full Code bisa di download References (http://mtkmudahbanar.blogspot.com/2016/08/metode-clustering-k-means-dan-fuzzy-c.html) (https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/) https://www.gurupendidikan.co.id/pengertian-data-menurut-para-ahli-serta-jenis-fungsi-dan-contoh/ https://www.jogjawebseo.com/pengertian-apa-itu-web-crawler/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ https://realpython.com/installing-python/ https://medium.com/@adamaulia/python-simple-crawling-using-beautifulsoup-8247657c2de5 https://agustiyadi.wordpress.com/2013/10/21/pentingnya-sebuah-data/ https://informatikalogi.com/text-preprocessing/ https://informatikalogi.com/term-weighting-tf-idf/ https://yudiagusta.wordpress.com/clustering/ https://datatofish.com/install-package-python-using-pip/ https://temukembaliinformasi.wordpress.com/2009/08/26/pembobotan-tf-idf/ https://arfianhidayat.com/algoritma-tf-idf https://devtrik.com/python/text-preprocessing-dengan-python-nltk/ https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/ https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf)","title":"Introduction"},{"location":"#introduction","text":"Menurut bahasa, data merupakan bentuk jamak dari kata datum (bahasa latin) yang berarti sesuatu yang diberikan . Menurut istilah, pengertian data adalah kumpulan informasi atau keterangan-keterangan yang diperoleh dari pengamatan, informasi itu bisa berupa angka, lambang atau sifat. Dalam kehidupan sehari-hari data berarti suatu pernyataan yang diterima secara apa adanya. Artinya data yang diperoleh dari berbagai sumbernya masih menjadi sebuah anggapan atau fakta karena memang belum diolah lebih lanjut. Setelah diolah melaui suatu penelitian atau percobaan maka data dapat berubah menjadi bentuk yang lebih kompleks misal database, informasi atau bahkan solusi pada masalah tertentu. Data yang telah diolah akan berubah menjadi sebuah informasi yang dapat digunakan untuk menambah pengetahuan bagi yang menerimanya. Maka dalam hal ini data dapat dianggap sebagai obyek dan informasi adalah suatu subyek yang bermanfaat bagi penerimanya. Informasi juga bisa disebut sebagai hasil pengolahan ataupun pemrosesan data. Oleh karena itu artikel ini akan membahas tutorial untuk mengolah data yang diperoleh dari crawling web sehingga diperoleh suatu informasi. Data yang didapat dari proses crawl akan diolah menggunakan metode Fuzzy C-Means untuk melakukan cluster pada data dan Silhouette Coefficient digunakan untuk menguji kualitas cluster yang diperoleh. Hasil akhir yang ingin dicapai pada artikel ini adalah membandingkan jumlah cluster sehingga mendapatkan jumlah cluster yang efiseien dan berkualitas berdasarkan perolehan skor dari Silhouette Coefficient . Selain itu pada tutorial ini juga terdapat proses Feature Selection untuk mengurangi jumlah fitur agar tidak memakan waktu komputasi yang banyak. Algoritma yang digunakan pada proses ini adalah Drop High Correlation , algortima ini digunakan untuk menghindari membuang fitur yang memiliki informasi yang diperlukan karena algoritma ini melakukan proses korelasi matrik untuk mengetahui fitur mana yang tidak memiliki korelasi dengan fitur yang lainnya. Sehingga algoritma ini dapat mencegah membuang fitur yang penting.","title":"Introduction"},{"location":"#step-1-installing","text":"Dalam tutorial ini semua proses akan dibangun menggunakan bahasa pemograman Python . Alasan mengapa author menggunakan bahasa pemograman ini karena Python merupakan bahasa pemograman yang populer akhir-akhir ini, selain itu Python menyediakan banyak library yang mudah digunakan dalam membangun suatu sistem. Adapun library yang digunakan akan dijelaskan pada setiap stepnya. Panduan instalasi python dapat teman-teman baca disini Panduan instalasi library atau package dapat teman-teman disini","title":"Step 1. Installing"},{"location":"#step-2-crawling","text":"","title":"Step 2. Crawling"},{"location":"#apa-itu-web-crawler","text":"Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Umumnya Web Crawler dapat digunakan berkaitan dengan Search Engine , yakni mengumpulkan informasi mengenai apa yang ada pada halaman-halaman web publik.","title":"Apa itu Web Crawler ?"},{"location":"#bagaimana-cara-kerja-web-crawler","text":"Ketika Web Crawler suatu Search Engine mengunjungi halaman web, Web Crawler akan membaca teks , hyperlink dan macam-macam tag konten yang digunakan dalam situs misalnya meta tag yang berisi banyak keyword. Data tersebut kemudian akan dimasukkan ke dalam database atau tempat penyimpanan.","title":"Bagaimana cara kerja Web Crawler ?"},{"location":"#apa-yang-akan-dilakukan","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan","text":"from bs4 import BeautifulSoup import requests BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web.","title":"Install library yang digunakan"},{"location":"#menentukan-halaman-web-yang-akan-di-crawl","text":"Pada artikel ini halaman web yang akan di crawl datanya adalah https://sports.okezone.com/.","title":"Menentukan halaman web yang akan di crawl"},{"location":"#menentukan-informasi-apa-yang-akan-di-simpan","text":"Setelah menentukan halaman web yang akan di crawl, selanjutnya berdasarkan informasi yang ada di halaman web diatas, data artikel atau berita yang akan di simpan pada tutorial ini berupa URL, judul dan isi berita .","title":"Menentukan informasi apa yang akan di simpan"},{"location":"#proses-crawling","text":">> Membuat requests pada halaman web yang dituju req = requests.get('http://lintasperistiwa.com/') >> Menentukan berita yang akan diambil dari halaman web Berita yang akan diambil adalah yang terletak di panel tengah pada list berita terkini, berita ini merupakan berita yang diupload tiap hari. Untuk dapat mengakses tag html dari list berita tersebut, dapat dilakukan dengan inspect element , yakni dengan cara : klik kanan pada halaman web pilih inspect maka akan muncul panel inspect di jendela sebelah kanan Selanjutnya pada panel inspect , cari tag html yang memuat list berita pada halaman web. Biasanya list berita memiliki nama class yang sama pada setiap berita. Pada halaman web diatas contohnya adalah class item-outer . news_links = soup.find_all(\"div\",{'class':'vel-frame-post-block'}) >>Mendapatkan judul dan alamat web dari isi artikel Setelah tag html list berita telah diperoleh, selanjutnya adalah mencari tag html yang memuat judul dan link dari judul tersebut. Hal ini dilakukan agar menelusuri isi dari setiap list berita yang terdapat pada halaman tersebut. Biasanya tag html yang memuat link terdapat attribut href . Dari list berita tersebut, jika ditelusuri lebih mendalam tag html yang memuat link berita memiliki class ellipsis2 . for idx,news in enumerate(news_links): #find news title title_news= news.find('a',{'class':'mod-judul-post'}).text #find urll news url_news = news.find('a',{'class':'mod-judul-post'}).get('href') Dilakukan perulangan untuk membaca semua link yang terdapat pada list berita. >>Membuat requests pada setiap berita #find news content in url req_news = requests.get(url_news) soup_news = BeautifulSoup(req_news.text, \"lxml\") Setelah mendapatkan link setiap berita, sama seperti langkah sebelumnya hal yang dilakukan adalah melakukan requests pada setiap berita. >> Mendapatkan isi artikel pada berita Sama seperti langkah sebelumnya, yang harus dilakukan selanjutnya adalah melakukan Inspect pada salah satu berita kemudiah mancari tag html yang mengandung isi dari berita tersebut. #find news content news_content = soup_news.find(\"div\",{'class':'entry-content'}) #find paragraph in news content p = news_content.find_all('p') content = ' '.join(item .text for item in p) news_content = content.encode('utf8','replace') Setelah mendapatkan isi dari suatu berita yang terdapat pada tag p selanjutnya menggabungkan isi semua tag tersbut menjadi sebuah string. Note : program diatas tidak dapat digunakan pada halaman web yang lain. Alasannya karena setiap web memiliki tag html yang berbeda. Jadi jika ingin melakukan crawling pada halaman web yang lain, teman-teman harus menyesuaikan dengan tag htmlnya dengan melakukan inspect yang telah dicontohkan diatas.","title":"Proses Crawling"},{"location":"#menyimpan-data-dalam-bentuk-excel","text":"woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] baris = 1 if(len(news_content)>5): sheet.cell(row=baris, column=1).value = baris sheet.cell(row=baris, column=2).value = url_news sheet.cell(row=baris, column=3).value = title_news sheet.cell(row=baris, column=4).value = str(news_content) baris+=1 woorkbook.save('data_crawling.xlsx') Libary yang digunakan untuk mengolah data menggunakan excel adalah openpyxl . Library ini sangat mudah untuk mengakses data pada excel, setelah melakukan inisialisasi woorkbook = openpyxl.Workbook() dan mengakses sheet yang akan digunakan sheet = woorkbook['Data'] , data dapat disimpan berdasarkan baris dan kolom pada excel ` sheet.cell(row=baris, column=1).value = baris sheet.cell(row=baris, column=2).value = url_news sheet.cell(row=baris, column=3).value = title_news sheet.cell(row=baris, column=4).value = str(news_content) baris+=1` . Setelah semua baris dan kolom sudah diberikan nilai maka selanjutnya adalah menyimpan file excel dengan woorkbook.save('data_crawling.xlsx') .","title":"Menyimpan data dalam bentuk excel"},{"location":"#cara-akses","text":"woorkbook.save('data_crawling.xlsx') url = 'http://lintasperistiwa.com/' crawl = crawl_lintasperistiwa(url) List akan disimpan dengan nama crawl di directory data_log sedangkan file excel akan disimpan di directory excel_log . Sehingga sebelum code dijalankan, buatlah kedua folder tersebut terlebih dahulu pada directory yang sama dengan file .py Notes : dalam beberapa kasus akan muncul pesan error lxml saat code dijalankan, jika demikian yang harus dilakukan adalah menginstall library lxml.","title":"Cara akses"},{"location":"#hasil","text":"Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca","title":"Hasil"},{"location":"#step-3-preprocessing","text":"","title":"Step 3. Preprocessing"},{"location":"#apa-itu-text-preprocessing","text":"Berdasarkan ketidak teraturan struktur data teks, maka proses sistem temu kembali informasi ataupun text mining memerlukan beberapa tahap awal yang pada intinya adalah mempersiapkan agar teks dapat diubah menjadi lebih terstruktur. Salah satu implementasi dari text mining adalah tahap Text Preprocessing. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi (1) case folding, (2) tokenizing, (3) filtering, dan (4) stemming.","title":"Apa itu Text Preprocessing ?"},{"location":"#apa-yang-akan-dilakukan_1","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan_1","text":"import openpyxl import re from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory import pandas as pd import numpy as np Library yang digunakan pada text preprocessing ialah re digunakan untuk symbol removing , nltk digunakan untuk tokenizing dan stopword, Sastrawi digunakan untuk stemming.","title":"Install library yang digunakan"},{"location":"#symbol-removing","text":"data2 = [] for i in range(len(data)): data2.append(re.sub(r'[^\\w]',' ',str(data[i])))","title":"Symbol Removing"},{"location":"#tokenizing","text":"data5 = [] woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data4)): token = data4[i].split() data5.append(token) sheet.cell(row=i+1, column=1).value = i+1 for j in range(len(token)): sheet.cell(row=i+1, column=2+j).value = token[j] woorkbook.save('tokenisasi.xlsx') Dokumen teks terdiri dari sekumpulan kalimat, proses tokenisasi memecah dokumen tersebut menjadi bagian-bagian kata yang disebut token. Contohnya kalimat \"saya menyapu halaman di depan rumah\" setelah ditokenisasi menjadi sebuah list [\"saya\", \"memakan\", \"nasi\", \"di\", \"dalam\", \"dapur\"] .","title":"Tokenizing"},{"location":"#stopword","text":"data3 = [] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): stop = stopword.remove(data2[i]) data3.append(stop) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = stop woorkbook.save('stopwords.xlsx') Selanjutnya ialah mengambil kata-kata yang dianggap penting dari hasil tokenization atau membuang kata-kata yang dianggap tidak terlalu mempunyai arti penting dalam proses text mining. Contohnya data [\"saya\", \"makan\", \"nasi\", \"di\", \"dalam\", \"dapur\"] menjadi [\"saya\", \"makan\", \"nasi\", \"dalam\", \"dapur\"] .","title":"Stopword"},{"location":"#stemming","text":"data4 = [] factory = StemmerFactory() stemmer = factory.create_stemmer() woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): katadasar = stemmer.stem(data3[i]) data4.append(katadasar) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = katadasar woorkbook.save('stemming.xlsx') Stemming bertujuan untuk mentransformasikan kata menjadi kata dasarnya (root word) dengan menghilangkan semua imbuhan kata. Contohnya data [\"saya\", \"makan\", \"nasi\", \"dalam\", \"dapur\"] menjadi [\"saya\", \"makan\", \"nasi\", \"dalam\", \"dapur\"] .","title":"Stemming"},{"location":"#cara-akses_1","text":"Sebelum melakukan proses diatas, terlebih dahulu membaca data hasil crawling yang telah disimpan dalam bentuk list dengan cara dibawah ini. def load_list(name): with open(\"data_logs/\"+name+\".txt\", \"rb\") as fp: # Unpickling b = pickle.load(fp) print(\"Success to load.....\") return b Selanjutnya, pergunakan semua function pada step ini secara bersama. print(\"Load data......\") data = load_list(\"data\") print(\"Lowercase......\") lowercase = lowercase(data) print(\"Remove symbol......\") symbol_remover = symbol_remover(lowercase) print(\"Tokenisasi......\") tokenisasi = tokenisasi(symbol_remover) print(\"Stopword......\") stopword_s = stopword_s(tokenisasi) print(\"Stemming......\") stemming = stemming(stopword_s) Pada hasil akhir dari tutorial ini, kita akan membandingkan uni-gram dan bi-gram. Uni-gram yakni hasil token merupakan tiap satu suku kata, sedangkan bi-gram hasil token menjadi 2 kata. Contoh bi-gram [\"saya makan\", \"makan nasi\", \"nasi dalam\", \"dalam dapur\"]. Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini","title":"Cara akses"},{"location":"#step-4-fiture-selection","text":"","title":"Step 4. Fiture Selection"},{"location":"#apa-itu-fiture-selection","text":"Pemilihan istilah untuk dijadikan indeks merupakan isu yang penting dalam sistem temu-kembali informasi. Selanjunya proses pemilihan istilah ini disebut dengan seleksi fitur (feature selection).","title":"Apa itu Fiture Selection ?"},{"location":"#mengapa-fiture-selection-penting","text":"Fitur seleksi dapat menyebabkan berkurangnya ukuran indeks sehingga proses retrieval suatu dokumen menjadi lebih cepat sebab jumlah indeks yang dicari menjadi lebih sedikit. Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks.","title":"Mengapa Fiture Selection penting ?"},{"location":"#apa-itu-drop-high-correlation","text":"Korelasi adalah istilah statistik yang dalam penggunaan umum mengacu pada seberapa dekat dua variabel untuk memiliki hubungan linier satu sama lain. Fitur dengan korelasi tinggi lebih linear dan karenanya memiliki efek yang hampir sama pada variabel dependen. Jadi, ketika dua fitur memiliki korelasi tinggi, kita dapat menjatuhkan ( drop ) salah satu dari dua fitur tersebut. Penghapusan fitur yang berbeda dari dataset akan memiliki efek yang berbeda pada nilai p untuk dataset. Kami dapat menghapus fitur yang berbeda dan mengukur nilai p di setiap kasus. Nilai-p yang diukur ini dapat digunakan untuk memutuskan apakah akan mempertahankan fitur atau tidak. Untuk informasi lebih lanjut mengenai perhitungan matrik korelasi bisa teman-teman baca disini","title":"Apa itu Drop High Correlation ?"},{"location":"#apa-yang-akan-dilakukan_2","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan_2","text":"import pandas as pd import numpy as np Library yang digunakan pada seleksi fitur ialah pandas digunakan untuk membuat data menjadi data frame dan numpy digunakan untuk menjadikan data menjadi matrix dan mengolahnya.","title":"Install library yang digunakan"},{"location":"#proses-fiture-selection","text":">> Mengubah data menjadi dataframe Data yang berbentuk array dirubah menjadi dataframe , yakni struktur data tabel dua dimensi yang memiliki label. Hal ini akan memudahkan dalam operasi baris dan kolom. df = pd.DataFrame(X) >> Membuat korelasi matriks Dari matriks korelasi ini dapat dilihat bahwa pada bagian diagonal, nilainya pasti = 1 sebab variabel tersebut berkorelasi sempurna positif dengan dirinya sendiri. Nilai korelasi antara pasangan variabel pada set data1 dan data2, menunjukkan nilai dalam range -1 dan 1. Ini disebabkan oleh standarisasi menggunakan standar deviasi sebagai pembagi pada saat perhitungan hasil korelasinya. corr_matrix = df.corr().abs() >> Menentukan fitur yang akan dihapus Setelah mendapatkan matrik kolerasi, selanjutnya menentukan kolom yang memiliki nilai lebih dari 0.95 . Nilai ini fleksibel, bergantung pada batas nilai fitur yang dianggap penting. to_drop = [column for column in upper.columns if any(upper[column] > 0.95)] >> Menghapus fitur Proses selanjutnya ialah menghapus fitur yang dianggap tidak penting berdasarkan nilai batas yang ditentukan pada proses sebelumnya. new_data = df.drop(df.columns[to_drop], axis=1)","title":"Proses Fiture Selection"},{"location":"#full-code","text":"import openpyxl import re from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory #n-gram def generate_ngrams(data,n): n_gram = [] for i in range(0,len(data)): tokens = data[i] ngrams = zip(*[tokens[i:] for i in range(n)]) n_gram.append([\" \".join(ngram) for ngram in ngrams]) return n_gram #read data from excel book = openpyxl.load_workbook('crawling.xlsx') sheet = book['Data'] row_count = sheet.max_row data = [] for i in range(row_count): data.append(sheet.cell(row=i+1, column=4).value) #symbol remove data2 = [] for i in range(len(data)): data2.append(re.sub(r'[^\\w]',' ',str(data[i]))) #stopword kata sambung data3 = [] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): stop = stopword.remove(data2[i]) data3.append(stop) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = stop woorkbook.save('stopwords.xlsx') #stemming kata imbuan data4 = [] factory = StemmerFactory() stemmer = factory.create_stemmer() #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data)): katadasar = stemmer.stem(data3[i]) data4.append(katadasar) sheet.cell(row=i+1, column=1).value = i+1 sheet.cell(row=i+1, column=2).value = katadasar woorkbook.save('stemming.xlsx') #tokenisasi kata pemisah data5 = [] #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(len(data4)): token = data4[i].split() data5.append(token) sheet.cell(row=i+1, column=1).value = i+1 for j in range(len(token)): sheet.cell(row=i+1, column=2+j).value = token[j] woorkbook.save('tokenisasi.xlsx') #n-gram data6 = generate_ngrams(data5,2) print(len(data5)) #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] for i in range(0,len(data6)): sheet.cell(row=i+1, column=1).value = i+1 for j in range(0,len(data6[i])): sheet.cell(row=i+1, column=2+j).value = data6[i][j] print(data6[i][j]) woorkbook.save('n_gram.xlsx') #VSM mencari data yang sama term = [] #create excel woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] j=2 for i in range(len(data5)): for word in data5[i]: if word not in term : term.append(word) sheet.cell(row=1, column=j).value = word j+=1 #count word in document for i in range(len(data5)): sheet.cell(row=2+i, column=1).value = i+1 j = 2 for word in term : sheet.cell(row=2+i, column=j).value = data5[i].count(word) j+=1 woorkbook.save('VSM.xlsx')","title":"Full Code"},{"location":"#cara-akses_2","text":"collecting_fiture merupakan label dari setiap kolom atau list yang terdiri dari semua kata yang terdapat pada semua berita dimana tidak ada kata yang sama pada list. Panjang collecting_fiture sama dengan panjang kolom vsm . Sedangkan vsm adalah banyaknya kemunculan kata yang ada di collecting_fiture pada setiap berita. Sehingga diperoleh matrik 2-dimensi. term = [] data7 = [] fitur = [] woorkbook = openpyxl.Workbook() woorkbook.create_sheet('Data') sheet = woorkbook['Data'] j=2 for i in range(len(data5)): for word in data5[i]: if word not in term : term.append(word) sheet.cell(row=1, column=j).value = word fitur.append(word) j+=1 for i in range(len(data5)): sheet.cell(row=2+i, column=1).value = i+1 j = 2 tamporary = [] for word in term : sheet.cell(row=2+i, column=j).value = data5[i].count(word) tamporary.append(data5[i].count(word)) j+=1 data7.append(tamporary) woorkbook.save('VSM.xlsx') Sehingga vsm dan collecting_fiture digunakan sebagai input drop_highly_correlation . #Uni Gram vsm = load_list(\"vsm\") collecting_fiture = load_list(\"collecting_fiture\") new_vsm,new_collecting_fiture = drop_highly_correlation(vsm,collecting_fiture) #Bi Gram vsm_n_gram = load_list(\"vsm_2\") collecting_fiture_n_gram = load_list(\"collecting_fiture_2\") new_vsm_n_gram,new_collecting_fiture_n_gram = drop_highly_correlation(vsm_n_gram,collecting_fiture_n_gram)","title":"Cara akses"},{"location":"#hasil_1","text":"Hasil dari seleksi fitur ini adalah jumlah fitur atau atribut atau kolom pada data, baik uni-gram dan bi-gram akan berkurang. Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini","title":"hasil"},{"location":"#step-5-tf-idf","text":"","title":"Step 5. TF-IDF"},{"location":"#apa-itu-tf-idf","text":"Metode TF-IDF merupakan metode untuk menghitung bobot setiap kata yang paling umum digunakan pada information retrieval. Metode ini juga terkenal efisien, mudah dan memiliki hasil yang akurat. Metode ini akan menghitung nilai Term Frequency (TF) dan Inverse Document Frequency (IDF) pada setiap token (kata) di setiap dokumen dalam korpus.","title":"Apa itu TF-IDF ?"},{"location":"#apa-yang-akan-dilakukan_3","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan_3","text":"import math Library yang digunakan ialah math yang digunakan untuk melakukan proses perhitungan.","title":"Install library yang digunakan"},{"location":"#tf","text":"TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. def tf(data,term): new_data=[] for i in range(len(data)): tempt = [] for word in term: tempt.append(data[i].count(word)) new_data.append(tempt) return new_data","title":"TF"},{"location":"#idf","text":"IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. def idf(data): new_data = [] for i in range(len(data[0])): count = 0 for j in range(len(data)): count+= int(data[j][i]) new_data.append(math.log10(len(data)/count)) return new_data","title":"IDF"},{"location":"#tf-idf","text":"def tf_idf(tf,idf): new_data = [] for i in range(len(tf)): tempt = [] for j in range(len(tf[i])): tempt.append(int(tf[i][j])*int(idf[j])) new_data.append(tempt) return new_data","title":"TF-IDF"},{"location":"#cara-akses_3","text":"#Uni Gram stemming = load_list(\"stemming\") collecting_fiture = load_list(\"new_collecting_fiture\") print(\"Load Tf....\") tf_a = tf(stemming,collecting_fiture) print(\"Load idf...\") idf_a = idf(tf_a) print(\"Load tf_idf...\") tf_idf_a = tf_idf(tf_a,idf_a,collecting_fiture) #Bi Gram n_gram = load_list(\"token_2\") collecting_fiture_n_gram = load_list(\"new_collecting_fiture_2\") print(\"Load Tf_n_gram....\") tf_n_gram = tf(n_gram,collecting_fiture_n_gram) print(\"Load idf_n_gram...\") idf_n_gram = idf(tf_n_gram) print(\"Load tf_idf_n_gram...\") tf_idf_n_gram = tf_idf(tf_n_gram,idf_n_gram,collecting_fiture_n_gram) Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini","title":"Cara akses"},{"location":"#step-6-clustering","text":"","title":"Step 6. Clustering"},{"location":"#apa-itu-clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain.","title":"Apa itu Clustering ?"},{"location":"#apa-itu-fuzzy-c-means","text":"Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Untuk informasi lebih lanjut mengenai perhitungan Fuzzy C-Means , silahkan baca disini","title":"Apa itu Fuzzy C-Means ?"},{"location":"#apa-itu-silhouette-coefficient","text":"ilhouette coefficient berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Untuk informasi lebih lanjut mengenai perhitungan silhoutte coeffisien , silahkan baca disini","title":"Apa itu Silhouette Coefficient ?"},{"location":"#apa-yang-akan-dilakukan_4","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan_4","text":"import numpy as np from sklearn.metrics import silhouette_score import skfuzzy as fuzz Library yang digunakan ialah numpy yang digunakan untuk melakukan proses perhitungan, sklearn yang digunakan untuk menghitung nilai silhouette dan skfuzzy yang digunakan untuk melakukan proses perhitungan fuzzy c-means .","title":"Install library yang digunakan"},{"location":"#fuzzy-c-means","text":"cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) Untuk menghitung fuzzy c-means telah disediakan oleh library python, adapun inputnya yakni : data berupa array 2d merupakan data yang akan dikelompokkan. jumlah cluster atau kelas yang diinginkan. array eksponensial batas error jumlah maksimum iterasi Sedangkan outputnya berupa : cntr merupakan pusat cluster u merupakan matriks partisi-c fuzzy akhir u0 merupakan tebakan awal pada fuzzy c-partisied matrix distant merupakan matriks jarak euclidian fObj menrupakan riwayat fungsi obyektif iterasi merupakan jumlah iterasi yang dijalankan fpc: merupakan koefisien partisi fuzzy akhir. Yang akan digunakan pada tutorial ini adalah u","title":"Fuzzy C-means"},{"location":"#silhouette-coefficient","text":"silhouette_score(data, membership, random_state=10) Setelah setiap data telah di cluster , selanjutnya adalah menghitung silhouette score . Silhouette score dihitung pada setiap cluster, baik itu untuk uni-gram maupun bi-gram .","title":"Silhouette Coefficient"},{"location":"#full-code_1","text":"def clustering(data): s_avg = [] for i in range(2,len(data)): cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) s_avg.append(silhouette_score(data, membership, random_state=10)) return s_avg","title":"Full Code"},{"location":"#cara-akses_4","text":"#Uni Gram tfidf = load_list(\"tf_idf\") s_avg = clustering(tfidf) #Bi Gram tfidf = load_list(\"tf_idf_2\") s_avg = clustering(tfidf)","title":"Cara akses"},{"location":"#hasil_2","text":"Setelah diperoleh silhouette score untuk masing-masing cluster pada uni-gram dan bi-gram dengan menggunakan perintah dibawah : #Uni Gram s_avg = func.load_list(\"s_avg\") print(s_avg.index(max(s_avg))+2) #Bi Gram s_avg = func.load_list(\"s_avg_2\") print(s_avg.index(max(s_avg))+2) Diperoleh bahwa cluster yang memiliki silhouette score paling tinggi adalah 2.","title":"Hasil"},{"location":"#conclusion","text":"Hasil yang diperoleh pada percobaan di atas mungkin bisa dikatakan tidak terlalu memiliki hasil yang baik, dan mungkin code line yang ada masih terlalu panjang. Meski tutorial ini masih banyak kekurangan, semoga dapat memberikan manfaat untuk teman-teman semua. Jika ada masukan atau perbaikan yang memberikan hasil yang lebih baik atau yang memiliki code program yang lebih pendek, jangan sungkan untuk memberikan saran kalian melalui email. Full Code bisa di download","title":"Conclusion"},{"location":"#references","text":"(http://mtkmudahbanar.blogspot.com/2016/08/metode-clustering-k-means-dan-fuzzy-c.html) (https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/) https://www.gurupendidikan.co.id/pengertian-data-menurut-para-ahli-serta-jenis-fungsi-dan-contoh/ https://www.jogjawebseo.com/pengertian-apa-itu-web-crawler/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ https://realpython.com/installing-python/ https://medium.com/@adamaulia/python-simple-crawling-using-beautifulsoup-8247657c2de5 https://agustiyadi.wordpress.com/2013/10/21/pentingnya-sebuah-data/ https://informatikalogi.com/text-preprocessing/ https://informatikalogi.com/term-weighting-tf-idf/ https://yudiagusta.wordpress.com/clustering/ https://datatofish.com/install-package-python-using-pip/ https://temukembaliinformasi.wordpress.com/2009/08/26/pembobotan-tf-idf/ https://arfianhidayat.com/algoritma-tf-idf https://devtrik.com/python/text-preprocessing-dengan-python-nltk/ https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/ https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf)","title":"References"}]}