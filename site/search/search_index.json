{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction PageRank adalah suatu sistem SCORING (antara 1 \u2013 10) yang diciptakan oleh founder Google (Larry Page) untuk memberikan penilaian seberapa tinggi score dari sebuah halaman website. Semakin tinggi score halaman website tersebut, maka artinya semakin bagus KUALITAS halaman website tersebut di mata Google. Secara sederhana, sistem scoring dari PageRank dihitung berdasarkan LINK POPULARITY, yaitu semakin banyak link yang mengarah ke sebuah website, maka akan semakin tinggi PR dari website tersebut. disini Step 1. Installing Dalam tutorial ini semua proses akan dibangun menggunakan bahasa pemograman Python . Alasan mengapa author menggunakan bahasa pemograman ini karena Python merupakan bahasa pemograman yang populer akhir-akhir ini, selain itu Python menyediakan banyak library yang mudah digunakan dalam membangun suatu sistem. Adapun library yang digunakan akan dijelaskan pada setiap stepnya. Panduan instalasi python dapat teman-teman baca disini Panduan instalasi library atau package dapat teman-teman disini Step 2. Crawling Apa itu Web Crawler ? Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Umumnya Web Crawler dapat digunakan berkaitan dengan Search Engine , yakni mengumpulkan informasi mengenai apa yang ada pada halaman-halaman web publik. Bagaimana cara kerja Web Crawler ? Ketika Web Crawler suatu Search Engine mengunjungi halaman web, Web Crawler akan membaca teks , hyperlink dan macam-macam tag konten yang digunakan dalam situs misalnya meta tag yang berisi banyak keyword. Data tersebut kemudian akan dimasukkan ke dalam database atau tempat penyimpanan. Apa yang akan dilakukan ? Install library yang digunakan from bs4 import BeautifulSoup import requests BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web. Menentukan halaman web yang akan di crawl Pada artikel ini halaman web yang akan di crawl datanya adalah http://lintasperistiwa.com/ . Menentukan informasi apa yang akan di simpan Setelah menentukan halaman web yang akan di crawl, selanjutnya berdasarkan informasi yang ada di halaman web diatas, data artikel atau berita yang akan di simpan pada tutorial ini berupa URL, judul dan isi berita . Proses Crawling >> Membuat requests pada halaman web yang dituju req = requests.get('http://lintasperistiwa.com/') >> Menentukan berita yang akan diambil dari halaman web Berita yang akan diambil adalah yang terletak di panel tengah pada list berita terkini, berita ini merupakan berita yang diupload tiap hari. Untuk dapat mengakses tag html dari list berita tersebut, dapat dilakukan dengan inspect element , yakni dengan cara : klik kanan pada halaman web pilih inspect maka akan muncul panel inspect di jendela sebelah kanan Selanjutnya pada panel inspect , cari tag html yang berisi link berita terkait pada tag html yang memuat tag <a news_links = soup.find_all('a',{'class':'mod-judul-post'}, href=True)e) >>Mendapatkan judul dan alamat web dari isi artikel Setelah tag html list berita telah diperoleh, selanjutnya adalah mencari tag html yang memuat judul dan link dari judul tersebut. Hal ini dilakukan agar menelusuri isi dari setiap list berita yang terdapat pada halaman tersebut. Biasanya tag html yang memuat link terdapat attribut href . Sehingga untuk melakukan crawling pada artikel tersebut, diperoleh code seperti berikut : req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) Looping hingga melakukan crawling pada kedalaman tertentu Full Code import requests from bs4 import BeautifulSoup def getLinks(url): req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'mod-judul-post'}, href=True) for link in news_links: listUrl.append(link['href']) n_depth = 3 swap_depth = 0 swap_list = 0 while swap_depth < n_depth : listUrl.append(\"depth\") next_depth = False while next_depth == False: if listUrl[swap_list] == \"depth\" : swap_list+=1 next_depth = True break if listUrl[swap_list].isdigit() : swap_list+=1 break listUrl.append(str(swap_list)) urls = listUrl[swap_list] reqs = requests.get(urls) soups = BeautifulSoup(reqs.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) for link in news_linkss: listUrl.append(link['href']) swap_list+=1 swap_depth+=1 if __name__== \"__main__\": listUrl = [] links = getLinks(\"http://lintasperistiwa.com/\") print(listUrl) Hasil : Step 2. Graph Processing Apa itu graph ? Graph atau Graf adalah kumpulan noktah (simpul) di dalam bidang dua dimensi yang dihubungkan dengan sekumpulan garis (sisi). Graph dapat digunakan untuk merepresentasikan objek-objek diskrit dan hubungan antara objek-objek tersebut. Representasi visual dari graph adalah dengan menyatakan objek sebagai noktah, bulatan atau titik (Vertex), sedangkan hubungan antara objek dinyatakan dengan garis (Edge). G = (V, E) Dimana : G = Graph V = Simpul atau Vertex, atau Node, atau Titik E = Busur atau Edge, atau arc V adalah himpunan verteks dan E adalah himpunan sisi yang terdefinisi antara pasangan-pasangan verteks. Sebuah sisi antara verteks x dan y ditulis {x,y}. Suatu graph H = (V1, E1) disebut subgraph dari graph G jika V1 adalah himpunan bagian dari V dan E1 himpunan bagian dari E. Cara pendefinisian lain untuk graph adalah dengan menggunakan himpunan keterhubungan langsung Vx. Pada setiap verteks x terdefinisi Vx sebagai himpunan dari verteks-verteks yang adjacent dari x. Secara formal: Vx = {y | (x,y) -> E} Apa manfaat dari graph ? Tiap-tiap diagram memuat sekumpulan obyek (kotak, titik, dan lain-lain) beserta garis-garis yang menghubungkan obyek-obyek tersebut. Garis bisa berarah ataupun tidak berarah. Garis yang berarah biasanya digunakan untuk menyatakan hubungan yang mementingkan urutan antar objek-objek. Urut-urutan objek akan mempunyai arti yang lain jika arah garis diubah. Sebagai contoh adalah garis komando yang menghubungkan titik-titik struktur sebuah organisasi. Sebaliknya, garis yang tidak berarah digunakan untuk menyatakan hubungan antar objek-objek yang tidak mementingkan urutan. Sebagai contoh adalah garis untuk menyatakan jarak hubung 2 kota pada Gambar 2. Jarak dari kota A ke kota B sejauh 200 km akan sama dengan jarak dari kota B ke kota A. Apabila jarak 2 tempat tidak sama jika dibalik (misalnya karena harus melalui jalan memutar), maka garis yang digunakan haruslah garis yang berarah. Apa yang akan dilakukan ? Install library yang digunakan import matplotlib.pyplot as plt import networkx as nx matplotlib digunakan untuk dapat memvisualisasikan graph , sedangkan networkx digunakan untuk dapat mengakses atau membangun graph . Menentukan node dan edge Node merupakan kumpulan dari link yang telah diperoleh dari hasil crawling, sedangkan Edge merupakan hubungan antar link atau link yang saling berkaitan. Oleh karena itu, saat melakukan crawling alangkah lebih baiknya jika menginisialisasi Node dan Edge secara bersamaan. urls = listUrl[swap_list] reqs = requests.get(urls) soups = BeautifulSoup(reqs.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) for link in news_linkss: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) value = (str(node.index(urls)),str(node.index(link['href']))) edge.append(value) Saat mengunjungi sebuah artikel, link dari artikel tersebut akan di tambahkan ke dalam list node node.append(link['href']) , setelah itu link dari artikel (str(node.index(urls)) dan link terkait str(node.index(link['href'])) di inisialisasi dan ditambakan pada list edge value = (str(node.index(urls)),str(node.index(link['href']))) edge.append(value) Sehingga keseluruhan code untuk proses crawling dan inisialisasi Node dan Edge full code : import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt def getLinks(url): req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'mod-judul-post'}, href=True) for link in news_links: listUrl.append(link['href']) node.append(link['href']) n_depth = 3 swap_depth = 0 swap_list = 0 while swap_depth < n_depth : listUrl.append(\"depth\") next_depth = False while next_depth == False: if listUrl[swap_list] == \"depth\" : swap_list+=1 next_depth = True swap_depth+=1 break try : urls = listUrl[swap_list] reqs = requests.get(urls) soups = BeautifulSoup(reqs.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) for link in news_linkss: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) value = (str(node.index(urls)),str(node.index(link['href']))) edge.append(value) except : pass swap_list+=1 swap_depth+=1 if __name__== \"__main__\": listUrl = [] node = [] edge = [] links = getLinks(\"http://lintasperistiwa.com/\") #print(listUrl) G=nx.DiGraph() pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) print(edge) print(type(edge)) #print(type(edge[0])) #membuat graf image = nx.draw_circular(G,node_color='blue', with_labels = True) plt.savefig(\"grap.png\", format=\"PNG\") #mancari nilai pagerank PR = nx.pagerank(G) print(PR) value = max(PR, key=PR.get) print(\"node : \" + max(PR,key=PR.get)) print(\"link : \" + node[int(value)]) nx.draw(G) plt.show() print(node) Hasil :","title":"Introduction"},{"location":"#introduction","text":"PageRank adalah suatu sistem SCORING (antara 1 \u2013 10) yang diciptakan oleh founder Google (Larry Page) untuk memberikan penilaian seberapa tinggi score dari sebuah halaman website. Semakin tinggi score halaman website tersebut, maka artinya semakin bagus KUALITAS halaman website tersebut di mata Google. Secara sederhana, sistem scoring dari PageRank dihitung berdasarkan LINK POPULARITY, yaitu semakin banyak link yang mengarah ke sebuah website, maka akan semakin tinggi PR dari website tersebut. disini","title":"Introduction"},{"location":"#step-1-installing","text":"Dalam tutorial ini semua proses akan dibangun menggunakan bahasa pemograman Python . Alasan mengapa author menggunakan bahasa pemograman ini karena Python merupakan bahasa pemograman yang populer akhir-akhir ini, selain itu Python menyediakan banyak library yang mudah digunakan dalam membangun suatu sistem. Adapun library yang digunakan akan dijelaskan pada setiap stepnya. Panduan instalasi python dapat teman-teman baca disini Panduan instalasi library atau package dapat teman-teman disini","title":"Step 1. Installing"},{"location":"#step-2-crawling","text":"","title":"Step 2. Crawling"},{"location":"#apa-itu-web-crawler","text":"Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Umumnya Web Crawler dapat digunakan berkaitan dengan Search Engine , yakni mengumpulkan informasi mengenai apa yang ada pada halaman-halaman web publik.","title":"Apa itu Web Crawler ?"},{"location":"#bagaimana-cara-kerja-web-crawler","text":"Ketika Web Crawler suatu Search Engine mengunjungi halaman web, Web Crawler akan membaca teks , hyperlink dan macam-macam tag konten yang digunakan dalam situs misalnya meta tag yang berisi banyak keyword. Data tersebut kemudian akan dimasukkan ke dalam database atau tempat penyimpanan.","title":"Bagaimana cara kerja Web Crawler ?"},{"location":"#apa-yang-akan-dilakukan","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan","text":"from bs4 import BeautifulSoup import requests BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web.","title":"Install library yang digunakan"},{"location":"#menentukan-halaman-web-yang-akan-di-crawl","text":"Pada artikel ini halaman web yang akan di crawl datanya adalah http://lintasperistiwa.com/ .","title":"Menentukan halaman web yang akan di crawl"},{"location":"#menentukan-informasi-apa-yang-akan-di-simpan","text":"Setelah menentukan halaman web yang akan di crawl, selanjutnya berdasarkan informasi yang ada di halaman web diatas, data artikel atau berita yang akan di simpan pada tutorial ini berupa URL, judul dan isi berita .","title":"Menentukan informasi apa yang akan di simpan"},{"location":"#proses-crawling","text":">> Membuat requests pada halaman web yang dituju req = requests.get('http://lintasperistiwa.com/') >> Menentukan berita yang akan diambil dari halaman web Berita yang akan diambil adalah yang terletak di panel tengah pada list berita terkini, berita ini merupakan berita yang diupload tiap hari. Untuk dapat mengakses tag html dari list berita tersebut, dapat dilakukan dengan inspect element , yakni dengan cara : klik kanan pada halaman web pilih inspect maka akan muncul panel inspect di jendela sebelah kanan Selanjutnya pada panel inspect , cari tag html yang berisi link berita terkait pada tag html yang memuat tag <a news_links = soup.find_all('a',{'class':'mod-judul-post'}, href=True)e) >>Mendapatkan judul dan alamat web dari isi artikel Setelah tag html list berita telah diperoleh, selanjutnya adalah mencari tag html yang memuat judul dan link dari judul tersebut. Hal ini dilakukan agar menelusuri isi dari setiap list berita yang terdapat pada halaman tersebut. Biasanya tag html yang memuat link terdapat attribut href . Sehingga untuk melakukan crawling pada artikel tersebut, diperoleh code seperti berikut : req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True)","title":"Proses Crawling"},{"location":"#looping-hingga-melakukan-crawling-pada-kedalaman-tertentu","text":"","title":"Looping hingga melakukan crawling pada kedalaman tertentu"},{"location":"#full-code","text":"import requests from bs4 import BeautifulSoup def getLinks(url): req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'mod-judul-post'}, href=True) for link in news_links: listUrl.append(link['href']) n_depth = 3 swap_depth = 0 swap_list = 0 while swap_depth < n_depth : listUrl.append(\"depth\") next_depth = False while next_depth == False: if listUrl[swap_list] == \"depth\" : swap_list+=1 next_depth = True break if listUrl[swap_list].isdigit() : swap_list+=1 break listUrl.append(str(swap_list)) urls = listUrl[swap_list] reqs = requests.get(urls) soups = BeautifulSoup(reqs.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) for link in news_linkss: listUrl.append(link['href']) swap_list+=1 swap_depth+=1 if __name__== \"__main__\": listUrl = [] links = getLinks(\"http://lintasperistiwa.com/\") print(listUrl) Hasil :","title":"Full Code"},{"location":"#step-2-graph-processing","text":"","title":"Step 2. Graph Processing"},{"location":"#apa-itu-graph","text":"Graph atau Graf adalah kumpulan noktah (simpul) di dalam bidang dua dimensi yang dihubungkan dengan sekumpulan garis (sisi). Graph dapat digunakan untuk merepresentasikan objek-objek diskrit dan hubungan antara objek-objek tersebut. Representasi visual dari graph adalah dengan menyatakan objek sebagai noktah, bulatan atau titik (Vertex), sedangkan hubungan antara objek dinyatakan dengan garis (Edge). G = (V, E) Dimana : G = Graph V = Simpul atau Vertex, atau Node, atau Titik E = Busur atau Edge, atau arc V adalah himpunan verteks dan E adalah himpunan sisi yang terdefinisi antara pasangan-pasangan verteks. Sebuah sisi antara verteks x dan y ditulis {x,y}. Suatu graph H = (V1, E1) disebut subgraph dari graph G jika V1 adalah himpunan bagian dari V dan E1 himpunan bagian dari E. Cara pendefinisian lain untuk graph adalah dengan menggunakan himpunan keterhubungan langsung Vx. Pada setiap verteks x terdefinisi Vx sebagai himpunan dari verteks-verteks yang adjacent dari x. Secara formal: Vx = {y | (x,y) -> E}","title":"Apa itu graph ?"},{"location":"#apa-manfaat-dari-graph","text":"Tiap-tiap diagram memuat sekumpulan obyek (kotak, titik, dan lain-lain) beserta garis-garis yang menghubungkan obyek-obyek tersebut. Garis bisa berarah ataupun tidak berarah. Garis yang berarah biasanya digunakan untuk menyatakan hubungan yang mementingkan urutan antar objek-objek. Urut-urutan objek akan mempunyai arti yang lain jika arah garis diubah. Sebagai contoh adalah garis komando yang menghubungkan titik-titik struktur sebuah organisasi. Sebaliknya, garis yang tidak berarah digunakan untuk menyatakan hubungan antar objek-objek yang tidak mementingkan urutan. Sebagai contoh adalah garis untuk menyatakan jarak hubung 2 kota pada Gambar 2. Jarak dari kota A ke kota B sejauh 200 km akan sama dengan jarak dari kota B ke kota A. Apabila jarak 2 tempat tidak sama jika dibalik (misalnya karena harus melalui jalan memutar), maka garis yang digunakan haruslah garis yang berarah.","title":"Apa manfaat dari graph ?"},{"location":"#apa-yang-akan-dilakukan_1","text":"","title":"Apa yang akan dilakukan ?"},{"location":"#install-library-yang-digunakan_1","text":"import matplotlib.pyplot as plt import networkx as nx matplotlib digunakan untuk dapat memvisualisasikan graph , sedangkan networkx digunakan untuk dapat mengakses atau membangun graph .","title":"Install library yang digunakan"},{"location":"#menentukan-node-dan-edge","text":"Node merupakan kumpulan dari link yang telah diperoleh dari hasil crawling, sedangkan Edge merupakan hubungan antar link atau link yang saling berkaitan. Oleh karena itu, saat melakukan crawling alangkah lebih baiknya jika menginisialisasi Node dan Edge secara bersamaan. urls = listUrl[swap_list] reqs = requests.get(urls) soups = BeautifulSoup(reqs.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) for link in news_linkss: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) value = (str(node.index(urls)),str(node.index(link['href']))) edge.append(value) Saat mengunjungi sebuah artikel, link dari artikel tersebut akan di tambahkan ke dalam list node node.append(link['href']) , setelah itu link dari artikel (str(node.index(urls)) dan link terkait str(node.index(link['href'])) di inisialisasi dan ditambakan pada list edge value = (str(node.index(urls)),str(node.index(link['href']))) edge.append(value) Sehingga keseluruhan code untuk proses crawling dan inisialisasi Node dan Edge full code : import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt def getLinks(url): req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'mod-judul-post'}, href=True) for link in news_links: listUrl.append(link['href']) node.append(link['href']) n_depth = 3 swap_depth = 0 swap_list = 0 while swap_depth < n_depth : listUrl.append(\"depth\") next_depth = False while next_depth == False: if listUrl[swap_list] == \"depth\" : swap_list+=1 next_depth = True swap_depth+=1 break try : urls = listUrl[swap_list] reqs = requests.get(urls) soups = BeautifulSoup(reqs.text, 'html.parser') news_linkss = soups.find_all('a',{'class':'vrp-thumb-link'}, href=True) for link in news_linkss: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) value = (str(node.index(urls)),str(node.index(link['href']))) edge.append(value) except : pass swap_list+=1 swap_depth+=1 if __name__== \"__main__\": listUrl = [] node = [] edge = [] links = getLinks(\"http://lintasperistiwa.com/\") #print(listUrl) G=nx.DiGraph() pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) print(edge) print(type(edge)) #print(type(edge[0])) #membuat graf image = nx.draw_circular(G,node_color='blue', with_labels = True) plt.savefig(\"grap.png\", format=\"PNG\") #mancari nilai pagerank PR = nx.pagerank(G) print(PR) value = max(PR, key=PR.get) print(\"node : \" + max(PR,key=PR.get)) print(\"link : \" + node[int(value)]) nx.draw(G) plt.show() print(node) Hasil :","title":"Menentukan node dan edge"}]}