# **Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx**

## **1. scrapy**

> scrapy menurut [wikipedia](https://en.wikipedia.org/wiki/Scrapy) Scrapy (/ˈskreɪpi/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk *web scrapping* dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd.

## 1.1 pemasangan scrapy

pastikan sudah memasang [python](https://www.python.org/) versi terbaru dan melakukan centang pada pip 

![gambar 1](D:\kampus\semester 8\penambangan web\tutorialCrawler\gambar\gambar 1.PNG)

agar bisa menggunakan perintah pip di cmd

setelah memasang python , maka install scrapy mengggunakan perintah `pip install scrapy`

jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung keadaan komputer masing masing 

## 1.2 membuat project scrapy

saya menggunakan project scrapy yang sama dengan tutorial sebelumnya , silahkan cek tutorial berikut ""   

## 1.3 mengatur domain

domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler ,

buka file **tugasAkhir.py** yang berada di folder spiders lalu atur `allowed_domains = ['id.wikipedia.org']` untuk membatasi link yang dijelajahi crawler tetap berada di domain `id.wikipedia.org`

## 1.4 mengatur startPage

startpage berguna memberi tahu  crawler link awal tempat dia mulai menjelajah seperti berikut `start_urls = ['https://id.wikipedia.org/wiki/Bank_Indonesia']` di skenario ini saya akan melakukan crawl **link internal** wikipedia dan akan dibuat graphnya untuk menghitung page rank masing masing halaman

## 1.5 mengatur data yang akan diambil

data yang akan diambil adalah link internal wikipedia indonesia yang akan disusun menjadi graph , karena saya tidak mengatur rule pada project ini maka kita gunakan method default scrapy  yaitu  `parse`  disini saya juga menggunakan bantuan beautifulsoup untuk antuan extraksi tag html , dan juga link wikipedia perlu di perbaiki karena jika di crawl langsung disimpan akan seperti ini 

![1558928886876](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1558928886876.png)

maka diperlukan perbaikan link terlebih dahulu sebelum disimpan dan dijelajahi



```python
    def parse(self, response):
        global situsKe
        situsKe+=1
        linkKe = 0
        deep = 1
        
        soup = BeautifulSoup(response.body , features="lxml")
        #inisialisasi beatifulsoup
        
        print("situsKe = ",situsKe ," url = ",response.url)
        links = soup.find_all('a') #select seluruh tag <a></a> di html
        
        linkDiperbaiki=list() #membuat list untuk memperbaiki link di wikipedia
        
        for x in links:
            tmp = str(x.get('href'))
            if(("/wiki/" in tmp[0:6]) and (":" not in tmp)):
   			#melakukan pengecekan apakah item link sesuai kriteria yaitu awalannya "/wiki/" karena inilah link internal yang akan dicari , dan tidak terdapat char ":" karena ini indikasi dia merujuk ke halaman itu sendiri 
                tmp1 = str("https://id.wikipedia.org" + tmp) #perbaikan link dengan menambahkan domain 
                linkDiperbaiki.append(tmp1)

        print("panjang links  = ",len(linkDiperbaiki))
        for x in linkDiperbaiki:     #proses simpan link    
            linkKe+=1
            print("linkKe = ",linkKe,"url = ",x)
            item = ItemTugasAkhir()
            item['url'] = response.url
            item['link_keluar'] = x
            item['situsKe'] = situsKe				
            item['linkKe'] = linkKe
            item['deep'] = deep
            yield item
        for x in linkDiperbaiki:  # menjelajahi link tsb 
            next_page = response.urljoin(x)
            yield scrapy.Request(next_page, callback=self.parse_deep2)
        print("===============================end deep1===========================")
```

copy sampai 3 def  parse3 agar crawler kita menjelajah sampai 3 kedalaman

## 1.6 crawl data

untuk crawling data bisa menggunakan perintah `scrapy crawl --nolog TugasAkhir -o data.csv -t csv`  data hasil crawling akan disimpan di data.csv 

proses crawling seperti berikut tunggu hingga selesai

**perintah sama dengan tutorial sebelumnya karena memang ini project sebellumnya yang dimodifikasi**

![1558929375374](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1558929375374.png)

setelah selesai data akan menjadi seperti berikut :

![1558929728350](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1558929728350.png)

## 2. migrasi csv ke sqlite

saya menggunakan tools [db browser for sqlite](https://sqlitebrowser.org/) disini sangat mudah  klik new database ketika muncul  pop up untuk mengisi field apa saja yang ada di db close saja 

![1555675137009](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1555675137009.png)

setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi

![1558929838634](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1558929838634.png)

tunggu hingga proses selesai 

 ![1555675509234](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1555675509234.png)

## 3. membuat graph menggunakan networkx

saya asumsikan data yang akan kita olah sudah ada di sqlite  , saya  membuat file bernama `networkx.py` yang digunakan untuk proses pembuatan graph dan menghitung pagerank



```python
import networkx as nx
import matplotlib.pyplot as plt
import sqlite3

def koneksi(db_file):
	try:
		conn = sqlite3.connect(db_file)
		return conn
	except Error as e:
		print(e)
	return None
def main(): 
    graph=nx.Graph() #inisialisasi networkx 
    node = set() #inisialisasi node

    database = "data.sqlite" # inisialisasi database dan membuat koneksi
    conn = koneksi(database)


    
```



## 3.1 menambahkan node

node adalah tiap halaman yang dikunjungi maupun link yang terdapat di halaman tsb , link yang terdapat di suatu halaman harus juga dibuat node nya agar saat penambahan edge tidak terjadi kesalahan karena edge merujuk ke suatu node yang tidak di inisialisasi

```python
def ambilNode(conn):
    node = set()
    
    cur = conn.cursor()
    cur.execute("select DISTINCT link_keluar from data") # mengambil link yang terkandung diseluruh halaman yang dikunjungi
    rows = cur.fetchall()
    for row in rows:
        tmp = str(row[0])
        node.add(tmp)
    
    cur = conn.cursor()
    cur.execute("select DISTINCT url from data") # mengambil halaman yang telah dikunjungi
    rows = cur.fetchall()
    for row in rows:
        tmp = str(row[0])
        node.add(tmp)
    
    return node
```

method diatas panggil di main dengan parameter koneksi tadi

```python
with conn: 
        node = ambilNode(conn)
graph.add_nodes_from(node)
```



## 3.2 menambahkan edge

menurut  [dokumentasi networkx](https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.Graph.add_edge.html) untuk menambahkan edge bisa menggunakan method `add_edges_from()` dengan parameter list yang berisi pasangan-pasangan node yang terhubung misalnya 

node a

node b

node c

untuk menghubungkan ketiga node list haruslah berbentuk `[[node a , node b],[node b , node c],[node c , node a]]` 



```python
def ambilEdge(conn):
    kumpulanEdge = list()
    primNode = list()
    
    cur = conn.cursor()
    cur.execute("select DISTINCT url from data") #select seluruh halaman yang pernah dikunjungi
    rows = cur.fetchall()
    for row in rows:
        
        tmp = str(row[0])
        primNode.append(tmp)
    
    for x in primNode:
        edge = list()
        cur = conn.cursor()
        query =  "select link_keluar from data where url='"+str(x)+"'" #select seluruh link yang terkandung di masing masing halaman yang pernah dikunjungi

        cur.execute(query)
        rows = cur.fetchall()
        for row in rows:
            tmp =[str(x),str(row[0])] 
            edge.append(tmp) #menambahkan pasangan - pasangan node
        kumpulanEdge.append(edge)#menambahkan seluruh pasangan suatu node ke Kumpulan edge , nantinya KumpulanEdge akan berisi seluruh node yang terhubung ke seluruh masing masing node
    return kumpulanEdge
```

fungsi tadi dipanggil di main dengan parameter koneksi , sehingga keseluruhan fungsi main seperti berikut , karena Kumpulanedge tadi berisi kumpulan maka harus dilakukan perulangan agar bisa menambahkan edge masing-masing node

```python
def main(): 
    graph=nx.Graph()
    node = set()
    database = "data.sqlite"
    conn = koneksi(database)
    with conn: 
        node = ambilNode(conn)
        kumpulanEdge = ambilEdge(conn)
    graph.add_nodes_from(node)
    for x in kumpulanEdge :
        # print("===== edge =====" , kumpulanEdge[x])
        graph.add_edges_from(x)
```

## 3.3 menampilkan graph

untuk menampilkan graph dibutuhkan matplotlib library bantuan untuk menampilkan graph cukup dengan fungsi berikut

```python
nx.draw(graph)
plt.show()
```

maka graph akan tampil

![1559223397569](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1559223397569.png)

## 4. menghitung pagerank dengan networkx

dari  graph yang telah disusun sebelumnya kita bisa menghitung pagerank menggunkan code berikut

```python
pr =  nx.pagerank(graph) #menghitung pagerank 
sorted_pr = sorted(pr.items() , reverse = True, key=lambda kv: kv[1]) # mengurutkan pagerank dari nilai yang terbesar

print("========== Top 3 pagerank ==========") #mencetak 3 pagerank tertinggi
print("1. ", sorted_pr[0])
print("2. ", sorted_pr[1])
print("3. ", sorted_pr[2])

```

![1559226072683](C:\Users\zainal\AppData\Roaming\Typora\typora-user-images\1559226072683.png)

full code ada di repo berikut  :